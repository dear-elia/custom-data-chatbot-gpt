import streamlit as st
from llama_index.core import VectorStoreIndex, Document
from llama_index.core.settings import Settings
from llama_index.llms.openai import OpenAI
import openai
from llama_index.core import SimpleDirectoryReader

openai.api_key = 'YOUR API KEY';
st.header("Chat with the Pai Chai chatbot ğŸ’¬ ğŸ“š")

if "messages" not in st.session_state.keys(): # Initialize the chat message history
    st.session_state.messages = [
        {"role": "assistant", "content": "Ask me a question about Streamlit's open-source Python library!"}
    ]

@st.cache_resource(show_spinner=False)
def load_data():
    with st.spinner(text="Loading and indexing the Streamlit docs â€“ hang tight! This should take 1-2 minutes."):
        reader = SimpleDirectoryReader(input_dir="C:/Users/PCU/VScode_project/data", recursive=True)
        docs = reader.load_data()
        
        # Set up the configuration directly in VectorStoreIndex if Settings cannot be directly instantiated
        index = VectorStoreIndex.from_documents(
            docs, 
            llm=OpenAI(model="gpt-3.5-turbo", temperature=0.5, 
                       system_prompt="ë‹¹ì‹ ì€ ëŒ€í•™ì˜ PDF ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” ëŠ¥ìˆ™í•œ ëŒ€í•™ ì¡°êµì…ë‹ˆë‹¤. "
                                      "ì‚¬ìš©ìê°€ í•œêµ­ì–´ë¡œ ì§ˆë¬¸í•˜ë©´ í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ê³ , ì˜ì–´ë¡œ ì§ˆë¬¸í•˜ë©´ ì˜ì–´ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. "
                                      "ì •í™•í•˜ê³  ê°„ê²°í•˜ë©° ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ëŠ” ë§¥ë½ ì¤‘ì‹¬ì˜ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤. "
                                      "ì¶©ë¶„í•œ ë§¥ë½ì´ ì£¼ì–´ì§„ ê²½ìš° í•´ë‹¹ ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ë¶€ì¡±í•  ê²½ìš° ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì‘ë‹µí•˜ì„¸ìš”. "
                                      "ë§¥ë½ì„ ì¶©ë¶„íˆ ë°˜ì˜í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, ë‹µë³€ì— ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ë˜, "
                                      "ì‚¬ìš©ìê°€ ì •í™•í•œ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ ê³µì‹ ì¶œì²˜ë¥¼ ê¶Œì¥í•˜ì‹­ì‹œì˜¤. "
                                      "ê°€ëŠ¥í•˜ë‹¤ë©´ ì„¸ ë¬¸ì¥ ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.")
        )
        return index

index = load_data()

chat_engine = index.as_chat_engine(chat_mode="condense_question", verbose=True)

if prompt := st.chat_input("Your question"): # Prompt for user input and save to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})

for message in st.session_state.messages: # Display the prior chat messages
    with st.chat_message(message["role"]):
        st.write(message["content"])

# If last message is not from assistant, generate a new response
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = chat_engine.chat(prompt)
            st.write(response.response)
            message = {"role": "assistant", "content": response.response}
            st.session_state.messages.append(message) # Add response to message history

